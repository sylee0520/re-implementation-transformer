{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizer import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tokenizer.Tokenizer at 0x10cdb61f0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = tokenizer.load_model()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[15953, 1, 373, 8, 5140]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = model.encode('I am a president')\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 1, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "import torch\n",
    "onehot = F.one_hot(torch.tensor(a), 16000)\n",
    "onehot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 16000])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onehot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "inputs = torch.rand(64, 20).long()\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EmbeddingLayer(\n",
       "  (embedding): Embedding(20, 512, padding_idx=0)\n",
       "  (linear): Embedding(512, 20, padding_idx=0)\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from model import EmbeddingLayer\n",
    "\n",
    "embed = EmbeddingLayer(num_embeddings=20, embedding_dim=512, padding_idx=0, layer_mode='embedding')\n",
    "embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 20, 512])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = embed(inputs)\n",
    "outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.7320508075688772"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.sqrt(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import *\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "from tokenizer import Tokenizer\n",
    "from datasets import Dataset\n",
    "num_embeddings=16000\n",
    "embedding_dim=512\n",
    "padding_idx=0\n",
    "model_dim=512\n",
    "q_dim=64\n",
    "k_dim=64\n",
    "v_dim=64\n",
    "n_heads=8\n",
    "in_dim=512\n",
    "hid_dim=2048\n",
    "out_dim=512\n",
    "max_seq_len=20\n",
    "N=6\n",
    "mask=True\n",
    "\n",
    "tokenizer = Tokenizer().load_model()\n",
    "dataset = Dataset(src='de', trg='en', data_type='train', tokenizer=tokenizer, max_seq_len=20)\n",
    "train_loader = DataLoader(dataset=dataset, batch_size=64, shuffle=False, collate_fn=dataset.collate_fn)\n",
    "embed = EmbeddingLayer(num_embeddings=16000, embedding_dim=512, padding_idx=0, layer_mode='embedding')\n",
    "encoder = Encoder(num_embeddings, embedding_dim, padding_idx, model_dim, q_dim, k_dim, v_dim, n_heads, in_dim, hid_dim, out_dim, max_seq_len, N)\n",
    "pe = PositionalEncodingLayer()\n",
    "mha = MultiHeadAttentionLayer(model_dim, q_dim, k_dim, v_dim, n_heads)\n",
    "decoder = Decoder(num_embeddings, embedding_dim, padding_idx, model_dim, q_dim, k_dim, v_dim, n_heads, in_dim, hid_dim, out_dim, max_seq_len, N, mask)\n",
    "decoderlayer = DecoderLayer(model_dim, q_dim, k_dim, v_dim, n_heads, in_dim, hid_dim, out_dim, mask)\n",
    "linear = EmbeddingLayer(num_embeddings=16000, embedding_dim=512, padding_idx=0, layer_mode='linear')\n",
    "transformer = Transformer(num_embeddings=16000, embedding_dim=512, padding_idx=0, \n",
    "                 model_dim=512, q_dim=64, k_dim=64, v_dim=64, n_heads=8,\n",
    "                 in_dim=512, hid_dim=2048, out_dim=512, max_seq_len=20, N=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input shape:  torch.Size([64, 20])\n",
      "output shape:  torch.Size([64, 20, 16000])\n",
      "tensor([[[5.5684e-05, 6.3741e-05, 1.3426e-04,  ..., 5.6626e-05,\n",
      "          1.1776e-04, 3.3381e-05],\n",
      "         [4.9182e-05, 4.0601e-05, 7.3506e-05,  ..., 5.9221e-05,\n",
      "          9.9536e-05, 1.8590e-05],\n",
      "         [1.0549e-04, 2.9490e-05, 5.7124e-05,  ..., 3.6819e-05,\n",
      "          8.1891e-05, 4.9782e-05],\n",
      "         ...,\n",
      "         [8.6015e-05, 4.1372e-05, 5.4584e-05,  ..., 6.0921e-05,\n",
      "          5.6543e-05, 2.2660e-05],\n",
      "         [9.8022e-05, 6.2315e-05, 2.9807e-05,  ..., 4.3203e-05,\n",
      "          9.1818e-05, 5.1485e-05],\n",
      "         [1.2017e-04, 2.0050e-05, 6.4837e-05,  ..., 7.4174e-05,\n",
      "          6.3432e-05, 4.1033e-05]],\n",
      "\n",
      "        [[4.8678e-05, 2.1104e-05, 4.1955e-05,  ..., 7.2610e-05,\n",
      "          4.0270e-05, 3.9668e-05],\n",
      "         [4.7566e-05, 4.2307e-05, 8.1006e-05,  ..., 7.5113e-05,\n",
      "          4.9947e-05, 4.7562e-05],\n",
      "         [3.6857e-05, 5.0062e-05, 8.4756e-05,  ..., 4.3505e-05,\n",
      "          7.5156e-05, 6.5367e-05],\n",
      "         ...,\n",
      "         [4.6847e-05, 4.3261e-05, 8.6301e-05,  ..., 9.3503e-05,\n",
      "          7.8919e-05, 6.2800e-05],\n",
      "         [9.8140e-05, 4.6454e-05, 7.8432e-05,  ..., 6.3527e-05,\n",
      "          4.6735e-05, 3.9535e-05],\n",
      "         [5.0284e-05, 7.4584e-05, 9.7117e-05,  ..., 7.2526e-05,\n",
      "          5.7495e-05, 7.1674e-05]],\n",
      "\n",
      "        [[3.3761e-05, 5.0502e-05, 8.3914e-05,  ..., 5.6166e-05,\n",
      "          6.4881e-05, 5.5876e-05],\n",
      "         [2.0573e-05, 4.9327e-05, 8.3801e-05,  ..., 6.7315e-05,\n",
      "          7.9362e-05, 4.7179e-05],\n",
      "         [1.1597e-05, 7.4538e-05, 9.8946e-05,  ..., 6.6646e-05,\n",
      "          4.7470e-05, 2.6869e-05],\n",
      "         ...,\n",
      "         [3.2568e-05, 5.9647e-05, 1.0751e-04,  ..., 5.6106e-05,\n",
      "          7.1956e-05, 4.7706e-05],\n",
      "         [4.1073e-05, 4.2563e-05, 4.1205e-05,  ..., 6.1314e-05,\n",
      "          6.6700e-05, 6.3410e-05],\n",
      "         [3.0269e-05, 3.0878e-05, 8.0742e-05,  ..., 6.3293e-05,\n",
      "          2.8468e-05, 3.8433e-05]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[2.7422e-05, 5.2101e-05, 2.1672e-05,  ..., 6.1578e-05,\n",
      "          4.6758e-05, 2.2539e-05],\n",
      "         [2.6068e-05, 4.9857e-05, 2.6244e-05,  ..., 4.8820e-05,\n",
      "          3.8457e-05, 1.7695e-05],\n",
      "         [3.2125e-05, 5.7656e-05, 2.2015e-05,  ..., 5.8459e-05,\n",
      "          3.7761e-05, 2.5023e-05],\n",
      "         ...,\n",
      "         [3.0204e-05, 2.8085e-05, 2.7698e-05,  ..., 5.6480e-05,\n",
      "          4.6527e-05, 2.3792e-05],\n",
      "         [3.2108e-05, 3.1006e-05, 3.4930e-05,  ..., 4.4073e-05,\n",
      "          5.0521e-05, 2.9941e-05],\n",
      "         [3.0243e-05, 3.2757e-05, 2.5990e-05,  ..., 5.1762e-05,\n",
      "          3.6596e-05, 2.3005e-05]],\n",
      "\n",
      "        [[4.5451e-05, 1.1503e-04, 6.8479e-05,  ..., 9.7106e-05,\n",
      "          4.7849e-05, 7.5262e-05],\n",
      "         [3.9806e-05, 1.3275e-04, 8.2149e-05,  ..., 7.6417e-05,\n",
      "          4.8793e-05, 3.8099e-05],\n",
      "         [6.0361e-05, 7.8366e-05, 9.8060e-05,  ..., 1.0233e-04,\n",
      "          2.6620e-05, 4.9711e-05],\n",
      "         ...,\n",
      "         [4.9833e-05, 8.7314e-05, 5.4010e-05,  ..., 7.2988e-05,\n",
      "          3.8964e-05, 4.9908e-05],\n",
      "         [5.2652e-05, 1.0845e-04, 5.4284e-05,  ..., 8.7131e-05,\n",
      "          3.8034e-05, 4.6812e-05],\n",
      "         [5.3927e-05, 8.0802e-05, 6.2816e-05,  ..., 8.0678e-05,\n",
      "          3.1303e-05, 4.3413e-05]],\n",
      "\n",
      "        [[2.7392e-05, 7.1413e-05, 3.2171e-04,  ..., 1.1450e-04,\n",
      "          5.1230e-05, 4.2196e-05],\n",
      "         [4.1790e-05, 5.0176e-05, 2.4836e-04,  ..., 7.7126e-05,\n",
      "          2.3731e-05, 5.6180e-05],\n",
      "         [2.0533e-05, 7.5364e-05, 2.0461e-04,  ..., 9.3601e-05,\n",
      "          3.3876e-05, 4.1437e-05],\n",
      "         ...,\n",
      "         [3.2671e-05, 4.4843e-05, 2.1603e-04,  ..., 7.0705e-05,\n",
      "          2.2898e-05, 4.9949e-05],\n",
      "         [3.3574e-05, 6.8884e-05, 2.2053e-04,  ..., 8.1387e-05,\n",
      "          2.5501e-05, 6.1675e-05],\n",
      "         [3.3219e-05, 5.2147e-05, 1.9961e-04,  ..., 1.0515e-04,\n",
      "          2.0147e-05, 8.2804e-05]]], grad_fn=<SoftmaxBackward>)\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1):\n",
    "    for i, data in enumerate(train_loader):\n",
    "        inputs, outputs = data\n",
    "        print(\"input shape: \", inputs.shape)\n",
    "        temp = transformer(inputs, outputs)\n",
    "        print(\"output shape: \", temp.shape)\n",
    "        print(temp)\n",
    "\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a38b83aeb1ea4339894febc55f8dc589b6819da101b0f16fbec59eab151f047a"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('tf-test')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
